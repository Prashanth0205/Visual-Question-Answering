{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "from transformers import ViTFeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import BertTokenizer, BertModel, ViTModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_dir , self.data_frame['filename'].iloc[index])\n",
    "        caption = self.data_frame['impression'].iloc[index]\n",
    "\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"Dataset\\Indiana University - Chest X-Rays\\images\\images\"\n",
    "image_caption_csv_path = \"Dataset\\Indiana University - Chest X-Rays\\indiana_chest_xray_captions.csv\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ChestXrayDataset(csv_file=image_caption_csv_path, img_dir=image_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextContrastive(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, hidden_size, projection_dim=256, momentum=0.999, lookup_size=65535):\n",
    "        super(ImageTextContrastive, self).__init__()\n",
    "        \n",
    "        # Shared image and text encoders\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "        # Projections to 256-dimensional vectors\n",
    "        self.image_projection = nn.Linear(hidden_size, projection_dim)\n",
    "        self.text_projection = nn.Linear(hidden_size, projection_dim)\n",
    "\n",
    "        # Momentum encoders (for MoCo-style momentum updates)\n",
    "        self.momentum_image_encoder = image_encoder  # Clone of image encoder for momentum updates\n",
    "        self.momentum_text_encoder = text_encoder    # Clone of text encoder for momentum updates\n",
    "        self.momentum_image_projection = nn.Linear(hidden_size, projection_dim)\n",
    "        self.momentum_text_projection = nn.Linear(hidden_size, projection_dim)\n",
    "        \n",
    "        # Initialize the momentum encoder weights to match the base encoder weights\n",
    "        self._init_momentum_encoders()\n",
    "        \n",
    "        # Lookup table for recent image-text representations\n",
    "        self.lookup_size = lookup_size\n",
    "        self.register_buffer(\"image_lookup\", torch.randn(lookup_size, projection_dim))\n",
    "        self.register_buffer(\"text_lookup\", torch.randn(lookup_size, projection_dim))\n",
    "        \n",
    "        # Momentum for updating momentum encoders\n",
    "        self.momentum = momentum\n",
    "        self.lookup_index = 0  # Pointer for updating the lookup table\n",
    "\n",
    "    def _init_momentum_encoders(self):\n",
    "        # Initialize momentum encoders to match the base encoders\n",
    "        for param_q, param_k in zip(self.image_encoder.parameters(), self.momentum_image_encoder.parameters()):\n",
    "            param_k.data.copy_(param_q.data)\n",
    "            param_k.requires_grad = False  # Momentum encoder is not trainable\n",
    "            \n",
    "        for param_q, param_k in zip(self.text_encoder.parameters(), self.momentum_text_encoder.parameters()):\n",
    "            param_k.data.copy_(param_q.data)\n",
    "            param_k.requires_grad = False\n",
    "        \n",
    "        # Initialize projection layers similarly\n",
    "        self.momentum_image_projection.load_state_dict(self.image_projection.state_dict())\n",
    "        self.momentum_text_projection.load_state_dict(self.text_projection.state_dict())\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_encoders(self):\n",
    "        # Apply momentum update to both image and text encoders\n",
    "        for param_q, param_k in zip(self.image_encoder.parameters(), self.momentum_image_encoder.parameters()):\n",
    "            param_k.data = param_k.data * self.momentum + param_q.data * (1.0 - self.momentum)\n",
    "        \n",
    "        for param_q, param_k in zip(self.text_encoder.parameters(), self.momentum_text_encoder.parameters()):\n",
    "            param_k.data = param_k.data * self.momentum + param_q.data * (1.0 - self.momentum)\n",
    "\n",
    "        # Update projection layers similarly\n",
    "        for param_q, param_k in zip(self.image_projection.parameters(), self.momentum_image_projection.parameters()):\n",
    "            param_k.data = param_k.data * self.momentum + param_q.data * (1.0 - self.momentum)\n",
    "\n",
    "        for param_q, param_k in zip(self.text_projection.parameters(), self.momentum_text_projection.parameters()):\n",
    "            param_k.data = param_k.data * self.momentum + param_q.data * (1.0 - self.momentum)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Image encoding with CLS token extraction\n",
    "        image_features = self.image_encoder(images).last_hidden_state[:, 0, :]  # CLS token\n",
    "        image_features = F.normalize(self.image_projection(image_features), dim=-1)\n",
    "\n",
    "        # Text encoding with CLS token extraction\n",
    "        text_features = self.extract_text_features(input_ids, attention_mask)\n",
    "        text_features = F.normalize(self.text_projection(text_features), dim=-1)\n",
    "\n",
    "        # Momentum encoding for contrastive loss calculation\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update_encoders()\n",
    "            momentum_image_features = self.momentum_image_encoder(images).last_hidden_state[:, 0, :]\n",
    "            momentum_image_features = F.normalize(self.momentum_image_projection(momentum_image_features), dim=-1)\n",
    "\n",
    "            momentum_text_features = self.extract_text_features(input_ids, attention_mask, momentum=True)\n",
    "            momentum_text_features = F.normalize(self.momentum_text_projection(momentum_text_features), dim=-1)\n",
    "\n",
    "            # Update lookup table\n",
    "            self._update_lookup_table(momentum_image_features, momentum_text_features)\n",
    "        \n",
    "        # Contrastive loss calculation\n",
    "        contrastive_loss = self.compute_contrastive_loss(image_features, text_features)\n",
    "        \n",
    "        return contrastive_loss\n",
    "\n",
    "    def extract_text_features(self, input_ids, attention_mask, momentum=False):\n",
    "        encoder = self.momentum_text_encoder if momentum else self.text_encoder\n",
    "        embeddings = encoder.embeddings(input_ids=input_ids)\n",
    "        \n",
    "        # Ensure attention_mask has the correct shape for broadcasting\n",
    "        attention_mask = attention_mask[:, None, None, :]  # Shape: (batch_size, 1, 1, sequence_length)\n",
    "        \n",
    "        # Apply only the first 6 layers of the encoder\n",
    "        text_features = embeddings\n",
    "        for layer in encoder.encoder.layer[:6]:\n",
    "            text_features = layer(text_features, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        return text_features[:, 0, :]  # CLS token\n",
    "\n",
    "    def _update_lookup_table(self, image_features, text_features):\n",
    "        # Update the lookup table with new entries using a circular index\n",
    "        batch_size = image_features.size(0)\n",
    "        if batch_size > self.lookup_size:\n",
    "            batch_size = self.lookup_size\n",
    "        \n",
    "        self.image_lookup[self.lookup_index:self.lookup_index + batch_size] = image_features[:batch_size]\n",
    "        self.text_lookup[self.lookup_index:self.lookup_index + batch_size] = text_features[:batch_size]\n",
    "        self.lookup_index = (self.lookup_index + batch_size) % self.lookup_size\n",
    "\n",
    "    def compute_contrastive_loss(self, image_features, text_features):\n",
    "        # Compute cosine similarities between image and text features\n",
    "        sim_i2t = torch.mm(image_features, self.text_lookup.T)  # Image to text similarity\n",
    "        sim_t2i = torch.mm(text_features, self.image_lookup.T)  # Text to image similarity\n",
    "\n",
    "        # Apply contrastive learning loss based on ALBEF's approach\n",
    "        labels = torch.arange(image_features.size(0), device=image_features.device)\n",
    "        loss_i2t = F.cross_entropy(sim_i2t, labels)\n",
    "        loss_t2i = F.cross_entropy(sim_t2i, labels)\n",
    "\n",
    "        return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextContrastive(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, hidden_size, projection_dim=256, momentum=0.995):\n",
    "        super(ImageTextContrastive, self).__init__()\n",
    "        self.image_encoder = image_encoder  # Shared image encoder\n",
    "        self.text_encoder = text_encoder  # Shared text encoder (e.g., BERT model)\n",
    "        self.projection_dim = projection_dim\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Projection layers for image and text features\n",
    "        self.image_projection = nn.Linear(hidden_size, projection_dim)\n",
    "        self.text_projection = nn.Linear(hidden_size, projection_dim)\n",
    "\n",
    "        # Momentum encoders (deep copy of the original encoders)\n",
    "        self.momentum_image_encoder = copy.deepcopy(self.image_encoder)\n",
    "        self.momentum_text_encoder = copy.deepcopy(self.text_encoder)\n",
    "        self.momentum_image_projection = nn.Linear(hidden_size, projection_dim)\n",
    "        self.momentum_text_projection = nn.Linear(hidden_size, projection_dim)\n",
    "\n",
    "        # Ensure the momentum encoders are initialized with the same weights as the main encoders\n",
    "        self._initialize_momentum_encoders()\n",
    "\n",
    "    def _initialize_momentum_encoders(self):\n",
    "        # Copy parameters from main encoders to momentum encoders\n",
    "        for param, momentum_param in zip(self.image_encoder.parameters(), self.momentum_image_encoder.parameters()):\n",
    "            momentum_param.data.copy_(param.data)\n",
    "            momentum_param.requires_grad = False\n",
    "\n",
    "        for param, momentum_param in zip(self.text_encoder.parameters(), self.momentum_text_encoder.parameters()):\n",
    "            momentum_param.data.copy_(param.data)\n",
    "            momentum_param.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update(self):\n",
    "        # Apply the momentum update to both image and text encoders\n",
    "        for param, momentum_param in zip(self.image_encoder.parameters(), self.momentum_image_encoder.parameters()):\n",
    "            momentum_param.data = self.momentum * momentum_param.data + (1 - self.momentum) * param.data\n",
    "\n",
    "        for param, momentum_param in zip(self.text_encoder.parameters(), self.momentum_text_encoder.parameters()):\n",
    "            momentum_param.data = self.momentum * momentum_param.data + (1 - self.momentum) * param.data\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Main encoding and projection\n",
    "        image_features = self.image_encoder(pixel_values=images).last_hidden_state[:, 0, :]  # CLS token\n",
    "        image_features = F.normalize(self.image_projection(image_features), dim=-1)\n",
    "\n",
    "        text_features = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        text_features = F.normalize(self.text_projection(text_features), dim=-1)\n",
    "\n",
    "        # Momentum encoding and projection for contrastive learning\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update()  # Update momentum encoders\n",
    "\n",
    "            momentum_image_features = self.momentum_image_encoder(pixel_values=images).last_hidden_state[:, 0, :]\n",
    "            momentum_image_features = F.normalize(self.momentum_image_projection(momentum_image_features), dim=-1)\n",
    "\n",
    "            momentum_text_features = self.momentum_text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "            momentum_text_features = F.normalize(self.momentum_text_projection(momentum_text_features), dim=-1)\n",
    "\n",
    "        # Contrastive loss between main and momentum features\n",
    "        contrastive_loss = self.calculate_contrastive_loss(image_features, text_features, momentum_image_features, momentum_text_features)\n",
    "        return contrastive_loss\n",
    "\n",
    "    def calculate_contrastive_loss(self, image_features, text_features, momentum_image_features, momentum_text_features):\n",
    "        # Compute similarities and contrastive loss\n",
    "        batch_size = image_features.size(0)\n",
    "        temperature = 0.07\n",
    "\n",
    "        # Similarity scores\n",
    "        sim_image_text = torch.mm(image_features, text_features.t()) / temperature\n",
    "        sim_image_momentum_text = torch.mm(image_features, momentum_text_features.t()) / temperature\n",
    "        sim_momentum_image_text = torch.mm(momentum_image_features, text_features.t()) / temperature\n",
    "\n",
    "        # Labels for contrastive loss\n",
    "        labels = torch.arange(batch_size).to(image_features.device)\n",
    "\n",
    "        # Contrastive loss across both directions\n",
    "        loss = F.cross_entropy(sim_image_text, labels) + F.cross_entropy(sim_image_momentum_text, labels) + F.cross_entropy(sim_momentum_image_text, labels)\n",
    "        return loss / 3  # Average the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss: 2.125472068786621\n"
     ]
    }
   ],
   "source": [
    "image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "itc_model = ImageTextContrastive(image_encoder=image_encoder, text_encoder=text_encoder, hidden_size=image_encoder.config.hidden_size).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example of computing contrastive loss for one batch\n",
    "for images, captions in data_loader:\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Tokenize captions\n",
    "    tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    input_ids, attention_mask = tokenized['input_ids'], tokenized['attention_mask']\n",
    "\n",
    "    # Forward pass\n",
    "    contrastive_loss = itc_model(images, input_ids, attention_mask)\n",
    "    print(f\"Contrastive Loss: {contrastive_loss.item()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_itc_model(itc_model, data_loader, tokenizer, device, num_epochs=3, learning_rate=1e-4, checkpoint_path=\"itc_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Train the Image-Text Contrastive model with checkpointing.\n",
    "\n",
    "    Parameters:\n",
    "    - itc_model: ImageTextContrastive instance\n",
    "    - data_loader: DataLoader instance with training data\n",
    "    - tokenizer: BertTokenizer instance\n",
    "    - device: torch.device, either 'cuda' or 'cpu'\n",
    "    - num_epochs: int, number of training epochs\n",
    "    - learning_rate: float, learning rate for optimizer\n",
    "    - checkpoint_path: str, path to save/load model checkpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = optim.Adam(itc_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Load checkpoint if it exists\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        itc_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        itc_model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = len(data_loader)\n",
    "        \n",
    "        for i, (images, captions) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Tokenize captions and move to device\n",
    "            tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass through the ITC model\n",
    "            optimizer.zero_grad()\n",
    "            contrastive_loss = itc_model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Backpropagation\n",
    "            contrastive_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss for reporting\n",
    "            total_loss += contrastive_loss.item()\n",
    "\n",
    "            # Print progress every 10 batches\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{batch_count}], Loss: {total_loss / (i + 1):.4f}\")\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': itc_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Batch [10/927], Loss: 2.1313\n",
      "Epoch [1/3], Batch [20/927], Loss: 2.1242\n",
      "Epoch [1/3], Batch [30/927], Loss: 2.1147\n",
      "Epoch [1/3], Batch [40/927], Loss: 2.1103\n",
      "Epoch [1/3], Batch [50/927], Loss: 2.1058\n",
      "Epoch [1/3], Batch [60/927], Loss: 2.1028\n",
      "Epoch [1/3], Batch [70/927], Loss: 2.0999\n",
      "Epoch [1/3], Batch [80/927], Loss: 2.0999\n",
      "Epoch [1/3], Batch [90/927], Loss: 2.0990\n",
      "Epoch [1/3], Batch [100/927], Loss: 2.0978\n",
      "Epoch [1/3], Batch [110/927], Loss: 2.0971\n",
      "Epoch [1/3], Batch [120/927], Loss: 2.0963\n",
      "Epoch [1/3], Batch [130/927], Loss: 2.0957\n",
      "Epoch [1/3], Batch [140/927], Loss: 2.0955\n",
      "Epoch [1/3], Batch [150/927], Loss: 2.0951\n",
      "Epoch [1/3], Batch [160/927], Loss: 2.0945\n",
      "Epoch [1/3], Batch [170/927], Loss: 2.0940\n",
      "Epoch [1/3], Batch [180/927], Loss: 2.0935\n",
      "Epoch [1/3], Batch [190/927], Loss: 2.0930\n",
      "Epoch [1/3], Batch [200/927], Loss: 2.0925\n",
      "Epoch [1/3], Batch [210/927], Loss: 2.0921\n",
      "Epoch [1/3], Batch [220/927], Loss: 2.0916\n",
      "Epoch [1/3], Batch [230/927], Loss: 2.0913\n",
      "Epoch [1/3], Batch [240/927], Loss: 2.0909\n",
      "Epoch [1/3], Batch [250/927], Loss: 2.0905\n",
      "Epoch [1/3], Batch [260/927], Loss: 2.0902\n",
      "Epoch [1/3], Batch [270/927], Loss: 2.0900\n",
      "Epoch [1/3], Batch [280/927], Loss: 2.0898\n",
      "Epoch [1/3], Batch [290/927], Loss: 2.0895\n",
      "Epoch [1/3], Batch [300/927], Loss: 2.0893\n",
      "Epoch [1/3], Batch [310/927], Loss: 2.0891\n",
      "Epoch [1/3], Batch [320/927], Loss: 2.0888\n",
      "Epoch [1/3], Batch [330/927], Loss: 2.0886\n",
      "Epoch [1/3], Batch [340/927], Loss: 2.0884\n",
      "Epoch [1/3], Batch [350/927], Loss: 2.0881\n",
      "Epoch [1/3], Batch [360/927], Loss: 2.0879\n",
      "Epoch [1/3], Batch [370/927], Loss: 2.0878\n",
      "Epoch [1/3], Batch [380/927], Loss: 2.0876\n",
      "Epoch [1/3], Batch [390/927], Loss: 2.0875\n",
      "Epoch [1/3], Batch [400/927], Loss: 2.0873\n",
      "Epoch [1/3], Batch [410/927], Loss: 2.0872\n",
      "Epoch [1/3], Batch [420/927], Loss: 2.0870\n",
      "Epoch [1/3], Batch [430/927], Loss: 2.0868\n",
      "Epoch [1/3], Batch [440/927], Loss: 2.0867\n",
      "Epoch [1/3], Batch [450/927], Loss: 2.0866\n",
      "Epoch [1/3], Batch [460/927], Loss: 2.0864\n",
      "Epoch [1/3], Batch [470/927], Loss: 2.0863\n",
      "Epoch [1/3], Batch [480/927], Loss: 2.0862\n",
      "Epoch [1/3], Batch [490/927], Loss: 2.0861\n",
      "Epoch [1/3], Batch [500/927], Loss: 2.0859\n",
      "Epoch [1/3], Batch [510/927], Loss: 2.0859\n",
      "Epoch [1/3], Batch [520/927], Loss: 2.0858\n",
      "Epoch [1/3], Batch [530/927], Loss: 2.0856\n",
      "Epoch [1/3], Batch [540/927], Loss: 2.0855\n",
      "Epoch [1/3], Batch [550/927], Loss: 2.0854\n",
      "Epoch [1/3], Batch [560/927], Loss: 2.0853\n",
      "Epoch [1/3], Batch [570/927], Loss: 2.0852\n",
      "Epoch [1/3], Batch [580/927], Loss: 2.0851\n",
      "Epoch [1/3], Batch [590/927], Loss: 2.0850\n",
      "Epoch [1/3], Batch [600/927], Loss: 2.0849\n",
      "Epoch [1/3], Batch [610/927], Loss: 2.0847\n",
      "Epoch [1/3], Batch [620/927], Loss: 2.0847\n",
      "Epoch [1/3], Batch [630/927], Loss: 2.0847\n",
      "Epoch [1/3], Batch [640/927], Loss: 2.0847\n",
      "Epoch [1/3], Batch [650/927], Loss: 2.0846\n",
      "Epoch [1/3], Batch [660/927], Loss: 2.0846\n",
      "Epoch [1/3], Batch [670/927], Loss: 2.0845\n",
      "Epoch [1/3], Batch [680/927], Loss: 2.0845\n",
      "Epoch [1/3], Batch [690/927], Loss: 2.0845\n",
      "Epoch [1/3], Batch [700/927], Loss: 2.0844\n",
      "Epoch [1/3], Batch [710/927], Loss: 2.0844\n",
      "Epoch [1/3], Batch [720/927], Loss: 2.0844\n",
      "Epoch [1/3], Batch [730/927], Loss: 2.0844\n",
      "Epoch [1/3], Batch [740/927], Loss: 2.0843\n",
      "Epoch [1/3], Batch [750/927], Loss: 2.0843\n",
      "Epoch [1/3], Batch [760/927], Loss: 2.0843\n",
      "Epoch [1/3], Batch [770/927], Loss: 2.0842\n",
      "Epoch [1/3], Batch [780/927], Loss: 2.0842\n",
      "Epoch [1/3], Batch [790/927], Loss: 2.0842\n",
      "Epoch [1/3], Batch [800/927], Loss: 2.0841\n",
      "Epoch [1/3], Batch [810/927], Loss: 2.0841\n",
      "Epoch [1/3], Batch [820/927], Loss: 2.0841\n",
      "Epoch [1/3], Batch [830/927], Loss: 2.0841\n",
      "Epoch [1/3], Batch [840/927], Loss: 2.0840\n",
      "Epoch [1/3], Batch [850/927], Loss: 2.0840\n",
      "Epoch [1/3], Batch [860/927], Loss: 2.0840\n",
      "Epoch [1/3], Batch [870/927], Loss: 2.0840\n",
      "Epoch [1/3], Batch [880/927], Loss: 2.0839\n",
      "Epoch [1/3], Batch [890/927], Loss: 2.0839\n",
      "Epoch [1/3], Batch [900/927], Loss: 2.0839\n",
      "Epoch [1/3], Batch [910/927], Loss: 2.0838\n",
      "Epoch [1/3], Batch [920/927], Loss: 2.0838\n",
      "Epoch [1/3], Loss: 2.0835\n",
      "Checkpoint saved at epoch 1\n",
      "Epoch [2/3], Batch [10/927], Loss: 2.0815\n",
      "Epoch [2/3], Batch [20/927], Loss: 2.0808\n",
      "Epoch [2/3], Batch [30/927], Loss: 2.0811\n",
      "Epoch [2/3], Batch [40/927], Loss: 2.0814\n",
      "Epoch [2/3], Batch [50/927], Loss: 2.0814\n",
      "Epoch [2/3], Batch [60/927], Loss: 2.0815\n",
      "Epoch [2/3], Batch [70/927], Loss: 2.0814\n",
      "Epoch [2/3], Batch [80/927], Loss: 2.0813\n",
      "Epoch [2/3], Batch [90/927], Loss: 2.0813\n",
      "Epoch [2/3], Batch [100/927], Loss: 2.0813\n",
      "Epoch [2/3], Batch [110/927], Loss: 2.0812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitc_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrain_itc_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitc_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitc_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mtrain_itc_model\u001b[1;34m(itc_model, data_loader, tokenizer, device, num_epochs, learning_rate, checkpoint_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m contrastive_loss \u001b[38;5;241m=\u001b[39m itc_model(images, input_ids, attention_mask)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mcontrastive_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Accumulate loss for reporting\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained models for image and text encoding\n",
    "image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Initialize tokenizer for text processing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "hidden_size = bert_model.config.hidden_size\n",
    "\n",
    "# Initialize the ITC model with shared encoders and hidden size\n",
    "itc_model = ImageTextContrastive(\n",
    "    image_encoder=image_encoder,\n",
    "    text_encoder=bert_model,\n",
    "    hidden_size=hidden_size,\n",
    "    projection_dim=256\n",
    ").to(device)\n",
    "\n",
    "# Prepare data loader\n",
    "# Assuming `data_loader` is already defined, containing pairs of images and captions\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-4\n",
    "checkpoint_path = \"itc_checkpoint.pth\"\n",
    "\n",
    "# Train the model\n",
    "train_itc_model(\n",
    "    itc_model=itc_model,\n",
    "    data_loader=data_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss on Test Batch: 2.079648733139038\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageTextContrastive' object has no attribute 'extract_text_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst Text Feature Vector:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Call the test function\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mtest_itc_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m, in \u001b[0;36mtest_itc_model\u001b[1;34m(itc_model, data_loader, tokenizer, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Extract image and text features for analysis\u001b[39;00m\n\u001b[0;32m     32\u001b[0m image_features \u001b[38;5;241m=\u001b[39m itc_model\u001b[38;5;241m.\u001b[39mimage_projection(\n\u001b[0;32m     33\u001b[0m     F\u001b[38;5;241m.\u001b[39mnormalize(itc_model\u001b[38;5;241m.\u001b[39mimage_encoder(images)\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m text_features \u001b[38;5;241m=\u001b[39m itc_model\u001b[38;5;241m.\u001b[39mtext_projection(\n\u001b[1;32m---> 36\u001b[0m     F\u001b[38;5;241m.\u001b[39mnormalize(\u001b[43mitc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text_features\u001b[49m(input_ids, attention_mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Print shapes and example features for verification\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage Features Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_features\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# Expected shape: (batch_size, projection_dim)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImageTextContrastive' object has no attribute 'extract_text_features'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_itc_model(itc_model, data_loader, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Test the trained Image-Text Contrastive model on a batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - itc_model: ImageTextContrastive instance (trained model)\n",
    "    - data_loader: DataLoader instance containing test data\n",
    "    - tokenizer: BertTokenizer instance for text processing\n",
    "    - device: torch.device, either 'cuda' or 'cpu'\n",
    "    \"\"\"\n",
    "    itc_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get a single batch of images and captions\n",
    "        images, captions = next(iter(data_loader))\n",
    "        \n",
    "        # Move images to the appropriate device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Tokenize captions and move to device\n",
    "        tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        input_ids, attention_mask = tokenized['input_ids'], tokenized['attention_mask']\n",
    "        \n",
    "        # Compute contrastive loss on the batch\n",
    "        contrastive_loss = itc_model(images, input_ids, attention_mask)\n",
    "        print(f\"Contrastive Loss on Test Batch: {contrastive_loss.item()}\")\n",
    "        \n",
    "        # Extract image and text features for analysis\n",
    "        image_features = itc_model.image_projection(\n",
    "            F.normalize(itc_model.image_encoder(images).last_hidden_state[:, 0, :], dim=-1)\n",
    "        )\n",
    "        text_features = itc_model.text_projection(\n",
    "            F.normalize(itc_model.extract_text_features(input_ids, attention_mask), dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Print shapes and example features for verification\n",
    "        print(f\"Image Features Shape: {image_features.shape}\")   # Expected shape: (batch_size, projection_dim)\n",
    "        print(f\"Text Features Shape: {text_features.shape}\")     # Expected shape: (batch_size, projection_dim)\n",
    "        \n",
    "        # Display the first image and text feature vectors to understand alignment\n",
    "        print(\"First Image Feature Vector:\", image_features[0].cpu().numpy())\n",
    "        print(\"First Text Feature Vector:\", text_features[0].cpu().numpy())\n",
    "\n",
    "# Call the test function\n",
    "test_itc_model(itc_model, data_loader, tokenizer, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
