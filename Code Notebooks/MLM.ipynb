{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import BertTokenizer, BertModel, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_dir , self.data_frame['filename'].iloc[index])\n",
    "        caption = self.data_frame['impression'].iloc[index]\n",
    "\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModeling(nn.Module):\n",
    "    def __init__(self, \n",
    "                 text_encoder_name='bert-base-uncased', \n",
    "                 vision_encoder_name='google/vit-base-patch16-224-in21k',\n",
    "                 mask_token_id=None,  # Pass mask_token_id from tokenizer\n",
    "                 pad_token_id=None,   # Pass pad_token_id from tokenizer\n",
    "                 mask_prob=0.15):\n",
    "        super(MaskedLanguageModeling, self).__init__()\n",
    "        \n",
    "        # Initialize BERT model and separate layers for text encoding and multimodal fusion\n",
    "        self.bert_model = BertModel.from_pretrained(text_encoder_name)\n",
    "        self.text_encoder_layers = nn.ModuleList(self.bert_model.encoder.layer[:6])  # First 6 layers for text encoding\n",
    "        self.multimodal_encoder_layers = nn.ModuleList(self.bert_model.encoder.layer[6:])  # Last 6 layers for multimodal fusion\n",
    "        \n",
    "        # Vision transformer for image encoding\n",
    "        self.vision_encoder = ViTModel.from_pretrained(vision_encoder_name)\n",
    "        \n",
    "        # Prediction head for MLM\n",
    "        self.prediction_head = nn.Linear(self.bert_model.config.hidden_size, self.bert_model.config.vocab_size)\n",
    "        \n",
    "        # Masking probability and token IDs\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def mask_tokens(self, input_ids):\n",
    "        \"\"\"\n",
    "        Masks input tokens with a probability of `mask_prob` and returns the masked input_ids and labels.\n",
    "        \"\"\"\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Ensure pad_token_id is on the same device as input_ids\n",
    "        pad_token_id = torch.tensor(self.pad_token_id, device=input_ids.device)\n",
    "        \n",
    "        # Create mask array for tokens to be masked\n",
    "        mask = (torch.rand(input_ids.shape, device=input_ids.device) < self.mask_prob) & (input_ids != pad_token_id)\n",
    "        \n",
    "        # Replace tokens with [MASK] token where mask is True\n",
    "        input_ids[mask] = self.mask_token_id\n",
    "        labels[~mask] = -100  # Set label to -100 for unmasked tokens, to ignore them in the loss\n",
    "        \n",
    "        return input_ids, labels\n",
    "\n",
    "    def forward(self, images, tokenized_text):\n",
    "        \"\"\"\n",
    "        Forward pass that processes images and tokenized text and outputs MLM predictions.\n",
    "        \"\"\"\n",
    "        input_ids, attention_mask = tokenized_text['input_ids'], tokenized_text['attention_mask']\n",
    "        \n",
    "        # 1. Image Encoding: Extract features from images using the ViT model\n",
    "        image_features = self.vision_encoder(images).last_hidden_state  # Shape: (batch_size, num_patches, hidden_size)\n",
    "        \n",
    "        # 2. Text Encoding: Apply masking on input_ids\n",
    "        masked_input_ids, labels = self.mask_tokens(input_ids)\n",
    "        \n",
    "        # Convert token IDs to embeddings using BERT's embedding layer\n",
    "        text_features = self.bert_model.embeddings(masked_input_ids)\n",
    "        \n",
    "        # Pass masked tokens through the first 6 layers of BERT for text encoding\n",
    "        for layer in self.text_encoder_layers:\n",
    "            text_features = layer(text_features)[0]\n",
    "        \n",
    "        # 3. Multimodal Interaction: Concatenate image and text features and pass through multimodal encoder\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)  # Concatenate along sequence dimension\n",
    "        multimodal_features = combined_features\n",
    "        for layer in self.multimodal_encoder_layers:\n",
    "            multimodal_features = layer(multimodal_features)[0]\n",
    "        \n",
    "        # 4. MLM Prediction: Apply prediction head to the text portion of the multimodal output\n",
    "        text_predictions = self.prediction_head(multimodal_features[:, -text_features.size(1):, :])  # Only text portion\n",
    "        \n",
    "        return text_predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"Dataset\\Indiana University - Chest X-Rays\\images\\images\"\n",
    "image_caption_csv_path = \"Dataset\\Indiana University - Chest X-Rays\\indiana_chest_xray_captions.csv\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ChestXrayDataset(csv_file=image_caption_csv_path, img_dir=image_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, caption = dataset.__getitem__(3)\n",
    "\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "plt.imshow(image)\n",
    "plt.title(f\"{caption}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "mlm_model = MaskedLanguageModeling(mask_token_id=mask_token_id, pad_token_id=pad_token_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlm_model = mlm_model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get one batch from the data loader\n",
    "images, captions = next(iter(data_loader))\n",
    "\n",
    "# Tokenize captions outside the model and move to device\n",
    "tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = tokenized['input_ids'].to(device)\n",
    "attention_mask = tokenized['attention_mask'].to(device)\n",
    "tokenized_text = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "images = images.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predictions, labels = mlm_model(images, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 37, 30522]) torch.Size([8, 37])\n"
     ]
    }
   ],
   "source": [
    "print(text_predictions.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predictions = text_predictions.view(-1, mlm_model.bert_model.config.vocab_size)\n",
    "labels = labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([296, 30522]) torch.Size([296])\n"
     ]
    }
   ],
   "source": [
    "print(text_predictions.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(text_predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predictions Shape: torch.Size([296, 30522])\n",
      "Labels Shape: torch.Size([296])\n",
      "Loss: 10.35284423828125\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Predictions Shape:\", text_predictions.shape)\n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [10/927], Loss: 9.709396171569825\n",
      "Batch [20/927], Loss: 9.182132697105407\n",
      "Batch [30/927], Loss: 8.739301300048828\n",
      "Batch [40/927], Loss: 8.268716651201249\n",
      "Batch [50/927], Loss: 7.971540026664734\n",
      "Batch [60/927], Loss: 7.609890321890513\n",
      "Batch [70/927], Loss: 7.3763805832181655\n",
      "Batch [80/927], Loss: 7.140872257947922\n",
      "Batch [90/927], Loss: 6.8168478833304516\n",
      "Batch [100/927], Loss: 6.57410493016243\n",
      "Batch [110/927], Loss: 6.3626073501326825\n",
      "Batch [120/927], Loss: 6.202050961057345\n",
      "Batch [130/927], Loss: 6.013554648252634\n",
      "Batch [140/927], Loss: 5.853852467026029\n",
      "Batch [150/927], Loss: 5.683526790936788\n",
      "Batch [160/927], Loss: 5.511126710660756\n",
      "Batch [170/927], Loss: 5.3891183251843735\n",
      "Batch [180/927], Loss: 5.28337623162402\n",
      "Batch [190/927], Loss: 5.173957423316805\n",
      "Batch [200/927], Loss: 5.10396466627717\n",
      "Batch [210/927], Loss: 5.01624285195555\n",
      "Batch [220/927], Loss: 4.921440846947107\n",
      "Batch [230/927], Loss: 4.827430165461872\n",
      "Batch [240/927], Loss: 4.756799542034666\n",
      "Batch [250/927], Loss: 4.6768128759861\n",
      "Batch [260/927], Loss: 4.612936664773867\n",
      "Batch [270/927], Loss: 4.53463342807911\n",
      "Batch [280/927], Loss: 4.448168520948717\n",
      "Batch [290/927], Loss: 4.371387677254348\n",
      "Batch [300/927], Loss: 4.331091070473194\n",
      "Batch [310/927], Loss: 4.276613424958721\n",
      "Batch [320/927], Loss: 4.230914453323931\n",
      "Batch [330/927], Loss: 4.197050350633535\n",
      "Batch [340/927], Loss: 4.156493114110302\n",
      "Batch [350/927], Loss: 4.123248183471816\n",
      "Batch [360/927], Loss: 4.080733757962784\n",
      "Batch [370/927], Loss: 4.043185004430848\n",
      "Batch [380/927], Loss: 4.008999323452774\n",
      "Batch [390/927], Loss: 3.9578670092500174\n",
      "Batch [400/927], Loss: 3.9156614910438656\n",
      "Batch [410/927], Loss: 3.8913330644005684\n",
      "Batch [420/927], Loss: 3.847900363838389\n",
      "Batch [430/927], Loss: 3.8163168603944224\n",
      "Batch [440/927], Loss: 3.7880650251087817\n",
      "Batch [450/927], Loss: 3.756932323674361\n",
      "Batch [460/927], Loss: 3.7286573243205963\n",
      "Batch [470/927], Loss: 3.704538838375122\n",
      "Batch [480/927], Loss: 3.6833483243671554\n",
      "Batch [490/927], Loss: 3.660081661933539\n",
      "Batch [500/927], Loss: 3.623411342948675\n",
      "Batch [510/927], Loss: 3.6100441761753137\n",
      "Batch [520/927], Loss: 3.588322660756799\n",
      "Batch [530/927], Loss: 3.5724588479354695\n",
      "Batch [540/927], Loss: 3.5498315804258542\n",
      "Batch [550/927], Loss: 3.519682078334418\n",
      "Batch [560/927], Loss: 3.4921354841174823\n",
      "Batch [570/927], Loss: 3.4638531204116974\n",
      "Batch [580/927], Loss: 3.4456130268501823\n",
      "Batch [590/927], Loss: 3.4253139527419867\n",
      "Batch [600/927], Loss: 3.3989187410473822\n",
      "Batch [610/927], Loss: 3.381180331062098\n",
      "Batch [620/927], Loss: 3.352817635093966\n",
      "Batch [630/927], Loss: 3.3362718422261497\n",
      "Batch [640/927], Loss: 3.3137651837430893\n",
      "Batch [650/927], Loss: 3.299385764323748\n",
      "Batch [660/927], Loss: 3.2874448553179247\n",
      "Batch [670/927], Loss: 3.263345712111957\n",
      "Batch [680/927], Loss: 3.2475606342887176\n",
      "Batch [690/927], Loss: 3.233718680082888\n",
      "Batch [700/927], Loss: 3.2212479496427946\n",
      "Batch [710/927], Loss: 3.209868264156328\n",
      "Batch [720/927], Loss: 3.2022152095619174\n",
      "Batch [730/927], Loss: 3.1920523084598047\n",
      "Batch [740/927], Loss: 3.178592336006664\n",
      "Batch [750/927], Loss: 3.1651996046602724\n",
      "Batch [760/927], Loss: 3.1505120588662594\n",
      "Batch [770/927], Loss: 3.1347257180550656\n",
      "Batch [780/927], Loss: 3.1174242068702975\n",
      "Batch [790/927], Loss: 3.106160114851745\n",
      "Batch [800/927], Loss: 3.099350964887999\n",
      "Batch [810/927], Loss: 3.083325614619218\n",
      "Batch [820/927], Loss: 3.067244533190458\n",
      "Batch [830/927], Loss: 3.0505959769823106\n",
      "Batch [840/927], Loss: 3.0341149370718217\n",
      "Batch [850/927], Loss: 3.02069691708421\n",
      "Batch [860/927], Loss: 2.997967873245131\n",
      "Batch [870/927], Loss: 2.982219471987979\n",
      "Batch [880/927], Loss: 2.970717935526574\n",
      "Batch [890/927], Loss: 2.9569707282389817\n",
      "Batch [900/927], Loss: 2.9502411447548202\n",
      "Batch [910/927], Loss: 2.943210655377134\n",
      "Batch [920/927], Loss: 2.9325948089280205\n",
      "Epoch [1/3], Loss: 2.9203\n",
      "Batch [10/927], Loss: 1.6898693829774856\n",
      "Batch [20/927], Loss: 1.7717447742819785\n",
      "Batch [30/927], Loss: 1.755114655693372\n",
      "Batch [40/927], Loss: 1.7123514480888844\n",
      "Batch [50/927], Loss: 1.7366201919317246\n",
      "Batch [60/927], Loss: 1.8129150306185087\n",
      "Batch [70/927], Loss: 1.8101447905812944\n",
      "Batch [80/927], Loss: 1.7817981988191605\n",
      "Batch [90/927], Loss: 1.753840070300632\n",
      "Batch [100/927], Loss: 1.7495650923252106\n",
      "Batch [110/927], Loss: 1.7508184243332257\n",
      "Batch [120/927], Loss: 1.752849998076757\n",
      "Batch [130/927], Loss: 1.7680227238398332\n",
      "Batch [140/927], Loss: 1.7428844901600054\n",
      "Batch [150/927], Loss: 1.7440532217919826\n",
      "Batch [160/927], Loss: 1.7174253415782004\n",
      "Batch [170/927], Loss: 1.6955398488132392\n",
      "Batch [180/927], Loss: 1.6988856463382642\n",
      "Batch [190/927], Loss: 1.7023009553159538\n",
      "Batch [200/927], Loss: 1.7055591829121113\n",
      "Batch [210/927], Loss: 1.6900403376136508\n",
      "Batch [220/927], Loss: 1.6855534129522063\n",
      "Batch [230/927], Loss: 1.678537834727246\n",
      "Batch [240/927], Loss: 1.6770054277032613\n",
      "Batch [250/927], Loss: 1.6733272442817688\n",
      "Batch [260/927], Loss: 1.666376106784894\n",
      "Batch [270/927], Loss: 1.666017836773837\n",
      "Batch [280/927], Loss: 1.6427383275968688\n",
      "Batch [290/927], Loss: 1.6296686994233007\n",
      "Batch [300/927], Loss: 1.6204524626458685\n",
      "Batch [310/927], Loss: 1.6147614869018716\n",
      "Batch [320/927], Loss: 1.6135136660304852\n",
      "Batch [330/927], Loss: 1.6174980386752973\n",
      "Batch [340/927], Loss: 1.608553507584421\n",
      "Batch [350/927], Loss: 1.5976808237816607\n",
      "Batch [360/927], Loss: 1.5895304060437614\n",
      "Batch [370/927], Loss: 1.587293351763809\n",
      "Batch [380/927], Loss: 1.57365320781736\n",
      "Batch [390/927], Loss: 1.5694028411729213\n",
      "Batch [400/927], Loss: 1.565751393418759\n",
      "Batch [410/927], Loss: 1.5593380870979006\n",
      "Batch [420/927], Loss: 1.5598508984914847\n",
      "Batch [430/927], Loss: 1.5500666809289954\n",
      "Batch [440/927], Loss: 1.5485513687472452\n",
      "Batch [450/927], Loss: 1.5423657716645134\n",
      "Batch [460/927], Loss: 1.5441981016941693\n",
      "Batch [470/927], Loss: 1.5398623987081204\n",
      "Batch [480/927], Loss: 1.529443759098649\n",
      "Batch [490/927], Loss: 1.5218381126316227\n",
      "Batch [500/927], Loss: 1.5152158623933791\n",
      "Batch [510/927], Loss: 1.5072889243855196\n",
      "Batch [520/927], Loss: 1.5067096051115256\n",
      "Batch [530/927], Loss: 1.5100092792848372\n",
      "Batch [540/927], Loss: 1.509618723558055\n",
      "Batch [550/927], Loss: 1.5081458338282325\n",
      "Batch [560/927], Loss: 1.5054649544081518\n",
      "Batch [570/927], Loss: 1.502522108115648\n",
      "Batch [580/927], Loss: 1.503403499414181\n",
      "Batch [590/927], Loss: 1.503854194131948\n",
      "Batch [600/927], Loss: 1.5007764573395252\n",
      "Batch [610/927], Loss: 1.495886392695982\n",
      "Batch [620/927], Loss: 1.4869497208345321\n",
      "Batch [630/927], Loss: 1.4815340470700038\n",
      "Batch [640/927], Loss: 1.4834010491613299\n",
      "Batch [650/927], Loss: 1.4722389794542239\n",
      "Batch [660/927], Loss: 1.4752875300180732\n",
      "Batch [670/927], Loss: 1.4638103721969162\n",
      "Batch [680/927], Loss: 1.462934544178493\n",
      "Batch [690/927], Loss: 1.4668371054357376\n",
      "Batch [700/927], Loss: 1.459762592752065\n",
      "Batch [710/927], Loss: 1.455956767446978\n",
      "Batch [720/927], Loss: 1.4527507765115135\n",
      "Batch [730/927], Loss: 1.4495563302648393\n",
      "Batch [740/927], Loss: 1.4503664070790685\n",
      "Batch [750/927], Loss: 1.456052290370067\n",
      "Batch [760/927], Loss: 1.4485428758455734\n",
      "Batch [770/927], Loss: 1.4455302713463059\n",
      "Batch [780/927], Loss: 1.4487325054426223\n",
      "Batch [790/927], Loss: 1.451275494873901\n",
      "Batch [800/927], Loss: 1.4490198012348265\n",
      "Batch [810/927], Loss: 1.4525930135982272\n",
      "Batch [820/927], Loss: 1.4496230846770652\n",
      "Batch [830/927], Loss: 1.4456021727118866\n",
      "Batch [840/927], Loss: 1.446782091783271\n",
      "Batch [850/927], Loss: 1.4430326320406268\n",
      "Batch [860/927], Loss: 1.4420211844752695\n",
      "Batch [870/927], Loss: 1.4425303931294502\n",
      "Batch [880/927], Loss: 1.4415556101721119\n",
      "Batch [890/927], Loss: 1.4390572623339262\n",
      "Batch [900/927], Loss: 1.4328783600115114\n",
      "Batch [910/927], Loss: 1.4308023851897036\n",
      "Batch [920/927], Loss: 1.4302824319299796\n",
      "Epoch [2/3], Loss: 1.4298\n",
      "Batch [10/927], Loss: 1.2031421780586242\n",
      "Batch [20/927], Loss: 1.2521701544523238\n",
      "Batch [30/927], Loss: 1.2040544730921587\n",
      "Batch [40/927], Loss: 1.2762152867391705\n",
      "Batch [50/927], Loss: 1.291791777163744\n",
      "Batch [60/927], Loss: 1.3006788985182842\n",
      "Batch [70/927], Loss: 1.3113504811057022\n",
      "Batch [80/927], Loss: 1.302196574304253\n",
      "Batch [90/927], Loss: 1.2900752778682443\n",
      "Batch [100/927], Loss: 1.2637608124874533\n",
      "Batch [110/927], Loss: 1.2583063376728785\n",
      "Batch [120/927], Loss: 1.2477074282088627\n",
      "Batch [130/927], Loss: 1.2390626700809941\n",
      "Batch [140/927], Loss: 1.2637684736733459\n",
      "Batch [150/927], Loss: 1.266163329463452\n",
      "Batch [160/927], Loss: 1.2540415996511\n",
      "Batch [170/927], Loss: 1.2701325109611978\n",
      "Batch [180/927], Loss: 1.2700007867326752\n",
      "Batch [190/927], Loss: 1.2744425094294314\n",
      "Batch [200/927], Loss: 1.2749009205726907\n",
      "Batch [210/927], Loss: 1.271201887757828\n",
      "Batch [220/927], Loss: 1.2349635590545156\n",
      "Batch [230/927], Loss: 1.232331203993248\n",
      "Batch [240/927], Loss: 1.2232028672782083\n",
      "Batch [250/927], Loss: 1.2108812130987645\n",
      "Batch [260/927], Loss: 1.2231580935132045\n",
      "Batch [270/927], Loss: 1.2151361324169017\n",
      "Batch [280/927], Loss: 1.2118133654019663\n",
      "Batch [290/927], Loss: 1.2166916465450976\n",
      "Batch [300/927], Loss: 1.2034688108662763\n",
      "Batch [310/927], Loss: 1.2070196485807818\n",
      "Batch [320/927], Loss: 1.2019950560759753\n",
      "Batch [330/927], Loss: 1.2007072019306095\n",
      "Batch [340/927], Loss: 1.2036777640528538\n",
      "Batch [350/927], Loss: 1.1996466009372047\n",
      "Batch [360/927], Loss: 1.1967444924275494\n",
      "Batch [370/927], Loss: 1.2183641876169555\n",
      "Batch [380/927], Loss: 1.2068713763973822\n",
      "Batch [390/927], Loss: 1.2166112704298053\n",
      "Batch [400/927], Loss: 1.2119556657364592\n",
      "Batch [410/927], Loss: 1.2056694543170856\n",
      "Batch [420/927], Loss: 1.2115750724316707\n",
      "Batch [430/927], Loss: 1.2149483059684552\n",
      "Batch [440/927], Loss: 1.2163484069899742\n",
      "Batch [450/927], Loss: 1.2256158998070492\n",
      "Batch [460/927], Loss: 1.2257972832890633\n",
      "Batch [470/927], Loss: 1.2221649145469704\n",
      "Batch [480/927], Loss: 1.2169279633361536\n",
      "Batch [490/927], Loss: 1.2123336467777892\n",
      "Batch [500/927], Loss: 1.2179592252336442\n",
      "Batch [510/927], Loss: 1.227483300305903\n",
      "Batch [520/927], Loss: 1.2322880317062999\n",
      "Batch [530/927], Loss: 1.2371355571520497\n",
      "Batch [540/927], Loss: 1.2291175946282844\n",
      "Batch [550/927], Loss: 1.225435814285143\n",
      "Batch [560/927], Loss: 1.2265312369480463\n",
      "Batch [570/927], Loss: 1.227076941999819\n",
      "Batch [580/927], Loss: 1.2286443650305015\n",
      "Batch [590/927], Loss: 1.2281312272299902\n",
      "Batch [600/927], Loss: 1.2278271034328887\n",
      "Batch [610/927], Loss: 1.2276709081666146\n",
      "Batch [620/927], Loss: 1.2228490439123445\n",
      "Batch [630/927], Loss: 1.222761620412625\n",
      "Batch [640/927], Loss: 1.2192858326685383\n",
      "Batch [650/927], Loss: 1.212600215108922\n",
      "Batch [660/927], Loss: 1.2086119266311552\n",
      "Batch [670/927], Loss: 1.2146693832088096\n",
      "Batch [680/927], Loss: 1.2117217435583691\n",
      "Batch [690/927], Loss: 1.2109587575376466\n",
      "Batch [700/927], Loss: 1.2100551699048707\n",
      "Batch [710/927], Loss: 1.2096496909594452\n",
      "Batch [720/927], Loss: 1.2068724194386353\n",
      "Batch [730/927], Loss: 1.2052978807371364\n",
      "Batch [740/927], Loss: 1.2075415784694457\n",
      "Batch [750/927], Loss: 1.2033790328452985\n",
      "Batch [760/927], Loss: 1.200740957157196\n",
      "Batch [770/927], Loss: 1.1986602056106286\n",
      "Batch [780/927], Loss: 1.1988740620668978\n",
      "Batch [790/927], Loss: 1.2004131494031016\n",
      "Batch [800/927], Loss: 1.1991782113129739\n",
      "Batch [810/927], Loss: 1.1994657850157424\n",
      "Batch [820/927], Loss: 1.1982004315717283\n",
      "Batch [830/927], Loss: 1.194962294721774\n",
      "Batch [840/927], Loss: 1.1963715829564967\n",
      "Batch [850/927], Loss: 1.1923618755522458\n",
      "Batch [860/927], Loss: 1.1918246493685645\n",
      "Batch [870/927], Loss: 1.1873959645703862\n",
      "Batch [880/927], Loss: 1.1854722336516716\n",
      "Batch [890/927], Loss: 1.1888208531167652\n",
      "Batch [900/927], Loss: 1.1856387135955608\n",
      "Batch [910/927], Loss: 1.187983807863074\n",
      "Batch [920/927], Loss: 1.1905120934228128\n",
      "Epoch [3/3], Loss: 1.1895\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "mlm_model = MaskedLanguageModeling(mask_token_id=mask_token_id, pad_token_id=pad_token_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlm_model = mlm_model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(mlm_model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    mlm_model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = len(data_loader)\n",
    "    for i, (images, captions) in enumerate(data_loader):\n",
    "        # Move images to the correct device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Tokenize captions outside the model and move to device\n",
    "        tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = tokenized['input_ids'].to(device)\n",
    "        attention_mask = tokenized['attention_mask'].to(device)\n",
    "        tokenized_text = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        text_predictions, labels = mlm_model(images, tokenized_text)\n",
    "        \n",
    "        # Reshape predictions and labels for calculating loss\n",
    "        text_predictions = text_predictions.view(-1, mlm_model.bert_model.config.vocab_size)\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = loss_fn(text_predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss for reporting\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print progress every 10 batches\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Batch [{i + 1}/{batch_count}], Loss: {total_loss / (i + 1)}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Caption: No acute pulmonary findings.\n",
      "Masked Caption: [MASK] no acute pulmonary [MASK]. [SEP]\n",
      "Predicted Tokens at Masked Positions: ['[CLS]', 'disease']\n"
     ]
    }
   ],
   "source": [
    "def test_sample(model, tokenizer, image, caption, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Tokenize the caption\n",
    "    tokenized = tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = tokenized['input_ids'].to(device)\n",
    "    attention_mask = tokenized['attention_mask'].to(device)\n",
    "    tokenized_text = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    # Move image to the device\n",
    "    image = image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Mask tokens in the caption (simulating the MLM task)\n",
    "    masked_input_ids, labels = model.mask_tokens(input_ids)\n",
    "    \n",
    "    # Pass the image and masked caption through the model\n",
    "    with torch.no_grad():\n",
    "        text_predictions, _ = model(image, {'input_ids': masked_input_ids, 'attention_mask': attention_mask})\n",
    "    \n",
    "    # Identify masked positions in the labels\n",
    "    masked_positions = (labels != -100).squeeze()\n",
    "    \n",
    "    # Get the predicted token IDs at the masked positions\n",
    "    predicted_token_ids = text_predictions.argmax(dim=-1).squeeze()[masked_positions]\n",
    "    \n",
    "    # Convert token IDs back to words\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids.tolist())\n",
    "    \n",
    "    # Print original and predicted tokens\n",
    "    print(\"Original Caption:\", caption)\n",
    "    print(\"Masked Caption:\", tokenizer.decode(masked_input_ids.squeeze()))\n",
    "    print(\"Predicted Tokens at Masked Positions:\", predicted_tokens)\n",
    "\n",
    "# Example usage:\n",
    "# Load a sample from the dataset (assuming `dataset` is an instance of ChestXrayDataset)\n",
    "sample_image, sample_caption = dataset[3]\n",
    "\n",
    "# Move model to device if not already done\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlm_model = mlm_model.to(device)\n",
    "\n",
    "# Test the model on a sample\n",
    "test_sample(mlm_model, tokenizer, sample_image, sample_caption, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlm_model(model, data_loader, tokenizer, device, num_epochs=3, learning_rate=1e-4, checkpoint_path=\"mlm_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Train the Masked Language Modeling model with checkpointing.\n",
    "\n",
    "    Parameters:\n",
    "    - model: MaskedLanguageModeling instance\n",
    "    - data_loader: DataLoader instance with training data\n",
    "    - tokenizer: BertTokenizer instance\n",
    "    - device: torch.device, either 'cuda' or 'cpu'\n",
    "    - num_epochs: int, number of training epochs\n",
    "    - learning_rate: float, learning rate for optimizer\n",
    "    - checkpoint_path: str, path to save/load model checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = len(data_loader)\n",
    "        \n",
    "        for i, (images, captions) in enumerate(data_loader):\n",
    "            # Move images to the correct device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Tokenize captions outside the model and move to device\n",
    "            tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "            tokenized_text = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            text_predictions, labels = model(images, tokenized_text)\n",
    "            \n",
    "            # Reshape predictions and labels for calculating loss\n",
    "            text_predictions = text_predictions.view(-1, model.bert_model.config.vocab_size)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            # Calculate loss and perform backpropagation\n",
    "            loss = loss_fn(text_predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss for reporting\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Print progress every 10 batches\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{batch_count}], Loss: {total_loss / (i + 1):.4f}\")\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer, model, and load checkpoint if available\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "mlm_model = MaskedLanguageModeling(mask_token_id=mask_token_id, pad_token_id=pad_token_id)\n",
    "mlm_model = mlm_model.to(device)\n",
    "\n",
    "# Assuming data_loader is already initialized with ChestXrayDataset\n",
    "train_mlm_model(mlm_model, data_loader, tokenizer, device, num_epochs=3, learning_rate=1e-4, checkpoint_path=\"models/mlm_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
