{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "from transformers import ViTFeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import BertTokenizer, BertModel, ViTModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_dir , self.data_frame['filename'].iloc[index])\n",
    "        caption = self.data_frame['impression'].iloc[index]\n",
    "\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"Dataset\\Indiana University - Chest X-Rays\\images\\images\"\n",
    "image_caption_csv_path = \"Dataset\\Indiana University - Chest X-Rays\\indiana_chest_xray_captions.csv\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ChestXrayDataset(csv_file=image_caption_csv_path, img_dir=image_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModeling(nn.Module):\n",
    "    def __init__(self, bert_model, image_encoder, mask_token_id, pad_token_id, mask_prob=0.15):\n",
    "        super(MaskedLanguageModeling, self).__init__()\n",
    "        \n",
    "        # Full BERT model to access embeddings directly\n",
    "        self.bert_model = bert_model\n",
    "        self.text_encoder_layers = nn.ModuleList(bert_model.encoder.layer[:6])  # First 6 layers for text encoding\n",
    "        self.multimodal_encoder_layers = nn.ModuleList(bert_model.encoder.layer[6:])  # Last 6 layers for multimodal fusion\n",
    "        \n",
    "        # Shared image encoder (e.g., Vision Transformer)\n",
    "        self.image_encoder = image_encoder\n",
    "\n",
    "        # Prediction head for MLM\n",
    "        hidden_size = bert_model.config.hidden_size\n",
    "        self.prediction_head = nn.Linear(hidden_size, bert_model.config.vocab_size)\n",
    "        \n",
    "        # Masking probability and token IDs\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def mask_tokens(self, input_ids):\n",
    "        labels = input_ids.clone()\n",
    "        mask = (torch.rand(input_ids.shape, device=input_ids.device) < self.mask_prob) & (input_ids != self.pad_token_id)\n",
    "        input_ids[mask] = self.mask_token_id\n",
    "        labels[~mask] = -100  # Ignore unmasked tokens in the loss\n",
    "        return input_ids, labels\n",
    "\n",
    "    def forward(self, images, tokenized_text):\n",
    "        input_ids, attention_mask = tokenized_text['input_ids'], tokenized_text['attention_mask']\n",
    "        \n",
    "        # Image Encoding\n",
    "        image_features = self.image_encoder(pixel_values=images).last_hidden_state  # Shape: (batch_size, num_patches, hidden_size)\n",
    "        \n",
    "        # Text Encoding with masking\n",
    "        masked_input_ids, labels = self.mask_tokens(input_ids)\n",
    "        \n",
    "        # Use embeddings directly from BERT\n",
    "        text_embeddings = self.bert_model.embeddings(masked_input_ids)\n",
    "        \n",
    "        # Pass through the first 6 layers of BERT for text encoding\n",
    "        for layer in self.text_encoder_layers:\n",
    "            text_embeddings = layer(text_embeddings)[0]\n",
    "        \n",
    "        # Multimodal Interaction\n",
    "        combined_features = torch.cat((image_features, text_embeddings), dim=1)  # Concatenate along sequence dimension\n",
    "        multimodal_features = combined_features\n",
    "        for layer in self.multimodal_encoder_layers:\n",
    "            multimodal_features = layer(multimodal_features)[0]\n",
    "        \n",
    "        # MLM Prediction\n",
    "        text_predictions = self.prediction_head(multimodal_features[:, -text_embeddings.size(1):, :])  # Only predict on text portion\n",
    "        \n",
    "        return text_predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Initialize the MLM model\n",
    "mlm_model = MaskedLanguageModeling(\n",
    "    bert_model=bert_model,\n",
    "    image_encoder=image_encoder,\n",
    "    mask_token_id=mask_token_id,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Predictions Shape: torch.Size([8, 15, 30522])\n",
      "Labels Shape: torch.Size([8, 15])\n"
     ]
    }
   ],
   "source": [
    "images, captions = next(iter(data_loader))\n",
    "images = images.to(device)\n",
    "tokenized_text = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Forward pass through the MLM model\n",
    "with torch.no_grad():\n",
    "    text_predictions, labels = mlm_model(images, tokenized_text)\n",
    "\n",
    "# Output shapes for verification\n",
    "print(\"Text Predictions Shape:\", text_predictions.shape)  # Expected: (batch_size, sequence_length, vocab_size)\n",
    "print(\"Labels Shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def train_mlm_model(mlm_model, bert_model, image_encoder, data_loader, tokenizer, device, num_epochs=3, learning_rate=1e-4, checkpoint_path=\"mlm_checkpoint.pth\", encoders_checkpoint_path=\"encoders_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Train the Masked Language Modeling model with shared encoders and checkpointing.\n",
    "\n",
    "    Parameters:\n",
    "    - mlm_model: MaskedLanguageModeling instance\n",
    "    - bert_model: Shared BERT model (used for text and multimodal encoding)\n",
    "    - image_encoder: Shared image encoder (Vision Transformer)\n",
    "    - data_loader: DataLoader instance with training data\n",
    "    - tokenizer: BertTokenizer instance\n",
    "    - device: torch.device, either 'cuda' or 'cpu'\n",
    "    - num_epochs: int, number of training epochs\n",
    "    - learning_rate: float, learning rate for optimizer\n",
    "    - checkpoint_path: str, path to save/load model checkpoint\n",
    "    - encoders_checkpoint_path: str, path to save/load encoder checkpoints\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = optim.Adam(mlm_model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load checkpoint if it exists\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        mlm_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        mlm_model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = len(data_loader)\n",
    "        \n",
    "        for i, (images, captions) in enumerate(data_loader):\n",
    "            # Move images to the correct device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Tokenize captions and move to device\n",
    "            tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "            tokenized_text = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            text_predictions, labels = mlm_model(images, tokenized_text)\n",
    "            \n",
    "            # Reshape predictions and labels for calculating loss\n",
    "            vocab_size = bert_model.config.vocab_size\n",
    "            text_predictions = text_predictions.view(-1, vocab_size)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(-1)  # Shape: (batch_size * seq_len)\n",
    "            \n",
    "            # Calculate loss and backpropagate\n",
    "            loss = loss_fn(text_predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss for reporting\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Print progress every 10 batches\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{batch_count}], Loss: {total_loss / (i + 1):.4f}\")\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save MLM model checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': mlm_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"MLM model checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "        # Save the shared encoders' states to enable reuse\n",
    "        torch.save({\n",
    "            'bert_model_state_dict': bert_model.state_dict(),\n",
    "            'image_encoder_state_dict': image_encoder.state_dict(),\n",
    "        }, encoders_checkpoint_path)\n",
    "        print(f\"Shared encoders' checkpoints saved at epoch {epoch + 1}\")\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Batch [10/927], Loss: 9.8781\n",
      "Epoch [1/3], Batch [20/927], Loss: 9.1913\n",
      "Epoch [1/3], Batch [30/927], Loss: 8.8698\n",
      "Epoch [1/3], Batch [40/927], Loss: 8.5506\n",
      "Epoch [1/3], Batch [50/927], Loss: 8.2074\n",
      "Epoch [1/3], Batch [60/927], Loss: 7.8312\n",
      "Epoch [1/3], Batch [70/927], Loss: 7.5788\n",
      "Epoch [1/3], Batch [80/927], Loss: 7.3073\n",
      "Epoch [1/3], Batch [90/927], Loss: 6.9983\n",
      "Epoch [1/3], Batch [100/927], Loss: 6.7616\n",
      "Epoch [1/3], Batch [110/927], Loss: 6.5130\n",
      "Epoch [1/3], Batch [120/927], Loss: 6.3386\n",
      "Epoch [1/3], Batch [130/927], Loss: 6.1512\n",
      "Epoch [1/3], Batch [140/927], Loss: 5.9870\n",
      "Epoch [1/3], Batch [150/927], Loss: 5.8531\n",
      "Epoch [1/3], Batch [160/927], Loss: 5.7231\n",
      "Epoch [1/3], Batch [170/927], Loss: 5.6108\n",
      "Epoch [1/3], Batch [180/927], Loss: 5.4668\n",
      "Epoch [1/3], Batch [190/927], Loss: 5.3388\n",
      "Epoch [1/3], Batch [200/927], Loss: 5.2250\n",
      "Epoch [1/3], Batch [210/927], Loss: 5.1146\n",
      "Epoch [1/3], Batch [220/927], Loss: 5.0460\n",
      "Epoch [1/3], Batch [230/927], Loss: 4.9454\n",
      "Epoch [1/3], Batch [240/927], Loss: 4.8598\n",
      "Epoch [1/3], Batch [250/927], Loss: 4.7877\n",
      "Epoch [1/3], Batch [260/927], Loss: 4.7003\n",
      "Epoch [1/3], Batch [270/927], Loss: 4.6429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_mlm_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlm_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlm_checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoders_checkpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoders_checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 60\u001b[0m, in \u001b[0;36mtrain_mlm_model\u001b[1;34m(mlm_model, bert_model, image_encoder, data_loader, tokenizer, device, num_epochs, learning_rate, checkpoint_path, encoders_checkpoint_path)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Calculate loss and backpropagate\u001b[39;00m\n\u001b[0;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(text_predictions, labels)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Accumulate loss for reporting\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_mlm_model(\n",
    "    mlm_model=mlm_model,\n",
    "    bert_model=bert_model,\n",
    "    image_encoder=image_encoder,\n",
    "    data_loader=data_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    num_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    checkpoint_path=\"mlm_checkpoint.pth\",\n",
    "    encoders_checkpoint_path=\"encoders_checkpoint.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
